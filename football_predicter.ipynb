{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "afd3bf03",
      "metadata": {},
      "source": [
        "# Premier League V4.5: Re-Optimizing for Draws\n",
        "\n",
        "A accuracy baixou porque mud√°mos as regras do jogo (pesos) mas mantivemos a estrat√©gia antiga.\n",
        "Nesta etapa, vamos correr o **Grid Search** novamente, mas desta vez informando o Grid Search de que os empates s√£o importantes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62edbbd4",
      "metadata": {},
      "source": [
        "Imports e Configura√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3950eca4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import joblib # Para salvar o modelo\n",
        "import re\n",
        "import os\n",
        "import codecs\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14553427",
      "metadata": {},
      "source": [
        "## 1. Data Acquisition (Recolha de Dados)\n",
        "Vamos buscar dados reais do `football-data.co.uk`. Vamos carregar v√°rias temporadas consecutivas para que o modelo tenha hist√≥rico suficiente para aprender padr√µes.\n",
        "\n",
        "* **FTHG**: Full Time Home Goals\n",
        "* **FTAG**: Full Time Away Goals\n",
        "* **FTR**: Full Time Result (H=Home, D=Draw, A=Away)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27652b81",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CONFIGURA√á√ÉO ---\n",
        "DATA_FILE = 'premier_league_full.csv'\n",
        "XG_FILE = 'premier_league_xg_data.csv'\n",
        "START_YEAR = 2005\n",
        "END_YEAR = 2025\n",
        "\n",
        "# --- FUN√á√ÉO 1: Scraper Robusto (Understat) ---\n",
        "def scrape_understat_season(year):\n",
        "    print(f\"üï∑Ô∏è A recolher xG de {year}/{year+1}...\")\n",
        "    url = f\"https://understat.com/league/EPL/{year}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        match = re.search(r\"datesData\\s*=\\s*JSON\\.parse\\('(.*?)'\\)\", response.text)\n",
        "        if not match:\n",
        "            print(f\"‚ö†Ô∏è Sem dados para {year}\")\n",
        "            return pd.DataFrame()\n",
        "            \n",
        "        json_data = codecs.decode(match.group(1), 'unicode_escape')\n",
        "        data = json.loads(json_data)\n",
        "        \n",
        "        matches = []\n",
        "        for m in data:\n",
        "            if m['isResult']:\n",
        "                matches.append({\n",
        "                    'Date': m['datetime'][:10],\n",
        "                    'HomeTeam': m['h']['title'],\n",
        "                    'AwayTeam': m['a']['title'],\n",
        "                    'Home_xG': float(m['xG']['h']),\n",
        "                    'Away_xG': float(m['xG']['a'])\n",
        "                })\n",
        "        return pd.DataFrame(matches)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erro no ano {year}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# --- FUN√á√ÉO 2: Carregar Dados Principais (Football-Data) ---\n",
        "def get_main_data(start, end):\n",
        "    if os.path.exists(DATA_FILE):\n",
        "        print(f\"üìÇ Carregando dados locais: {DATA_FILE}\")\n",
        "        df = pd.read_csv(DATA_FILE)\n",
        "        # Importante: N√£o converter data aqui ainda para controlar formato no main\n",
        "        return df\n",
        "    \n",
        "    print(\"üåê A descarregar dados do Football-Data...\")\n",
        "    dfs = []\n",
        "    base_url = \"https://www.football-data.co.uk/mmz4281/{}/{}.csv\"\n",
        "    for year in range(start, end + 1):\n",
        "        season = f\"{str(year)[-2:]}{str(year+1)[-2:]}\"\n",
        "        try:\n",
        "            df = pd.read_csv(base_url.format(season, \"E0\"))\n",
        "            # For√ßar convers√£o imediata para evitar problemas de mistura\n",
        "            df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n",
        "            dfs.append(df)\n",
        "        except: pass\n",
        "        \n",
        "    full_df = pd.concat(dfs, ignore_index=True).dropna(subset=['Date', 'FTR'])\n",
        "    full_df.to_csv(DATA_FILE, index=False)\n",
        "    return full_df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "# --- FUN√á√ÉO 3: Limpeza de Nomes ---\n",
        "def clean_team_name(name):\n",
        "    name_map = {\n",
        "        'Manchester United': 'Man United', 'Manchester City': 'Man City',\n",
        "        'Newcastle United': 'Newcastle', 'West Ham United': 'West Ham', 'West Ham': 'West Ham',\n",
        "        'Wolverhampton Wanderers': 'Wolves', 'Brighton': 'Brighton',\n",
        "        'Leicester City': 'Leicester', 'Leeds United': 'Leeds',\n",
        "        'Tottenham Hotspur': 'Tottenham', 'Tottenham': 'Tottenham', \n",
        "        'Nottingham Forest': \"Nott'm Forest\", 'Sheffield United': 'Sheffield United', \n",
        "        'Luton': 'Luton', 'Brentford': 'Brentford', 'Bournemouth': 'Bournemouth',\n",
        "        'Ipswich Town': 'Ipswich', 'Hull City': 'Hull', 'Stoke City': 'Stoke',\n",
        "        'Swansea City': 'Swansea', 'Cardiff City': 'Cardiff',\n",
        "        'Huddersfield Town': 'Huddersfield', 'West Bromwich Albion': 'West Brom',\n",
        "        'Norwich City': 'Norwich', 'Queens Park Rangers': 'QPR'\n",
        "    }\n",
        "    return name_map.get(name, name)\n",
        "\n",
        "# ==========================================\n",
        "# üöÄ EXECU√á√ÉO E LIMPEZA (A PARTE CR√çTICA)\n",
        "# ==========================================\n",
        "\n",
        "# 1. Carregar Dados Principais\n",
        "df = get_main_data(START_YEAR, END_YEAR)\n",
        "\n",
        "# Limpeza de Datas e Duplicados no Dataset Principal\n",
        "df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n",
        "df = df.dropna(subset=['Date'])\n",
        "# Remove duplicados exatos no ficheiro principal\n",
        "df = df.drop_duplicates(subset=['Date', 'HomeTeam', 'AwayTeam'])\n",
        "\n",
        "# 2. Carregar ou Sacar xG\n",
        "if os.path.exists(XG_FILE):\n",
        "    print(\"üìÇ Carregando xG local...\")\n",
        "    df_xg = pd.read_csv(XG_FILE)\n",
        "else:\n",
        "    print(\"üåê A iniciar scraping xG...\")\n",
        "    dfs_xg = [scrape_understat_season(y) for y in range(START_YEAR, END_YEAR)]\n",
        "    df_xg = pd.concat(dfs_xg, ignore_index=True)\n",
        "    df_xg['HomeTeam'] = df_xg['HomeTeam'].apply(clean_team_name)\n",
        "    df_xg['AwayTeam'] = df_xg['AwayTeam'].apply(clean_team_name)\n",
        "    df_xg.to_csv(XG_FILE, index=False)\n",
        "\n",
        "# 3. PREPARA√á√ÉO PARA MERGE\n",
        "df_xg['Date'] = pd.to_datetime(df_xg['Date']).dt.normalize()\n",
        "df['Date'] = df['Date'].dt.normalize()\n",
        "\n",
        "# --- CORRE√á√ÉO: Remover duplicados no xG ANTES do Merge ---\n",
        "print(f\"üìä Linhas xG antes da limpeza: {len(df_xg)}\")\n",
        "df_xg = df_xg.drop_duplicates(subset=['Date', 'HomeTeam', 'AwayTeam'], keep='first')\n",
        "print(f\"üìâ Linhas xG limpas: {len(df_xg)}\")\n",
        "\n",
        "# Aplicar limpeza de nomes\n",
        "df['HomeTeam'] = df['HomeTeam'].apply(clean_team_name)\n",
        "df['AwayTeam'] = df['AwayTeam'].apply(clean_team_name)\n",
        "df_xg['HomeTeam'] = df_xg['HomeTeam'].apply(clean_team_name)\n",
        "df_xg['AwayTeam'] = df_xg['AwayTeam'].apply(clean_team_name)\n",
        "\n",
        "# Remover colunas antigas de xG no DF principal\n",
        "cols_exclude = [c for c in df.columns if 'xG' in c]\n",
        "df_clean = df.drop(columns=cols_exclude)\n",
        "\n",
        "# 4. MERGE FINAL\n",
        "print(\"üîÑ A realizar o Merge...\")\n",
        "df_final = df_clean.merge(\n",
        "    df_xg[['Date', 'HomeTeam', 'AwayTeam', 'Home_xG', 'Away_xG']],\n",
        "    on=['Date', 'HomeTeam', 'AwayTeam'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# 5. REMOVER FUTURO (Seguran√ßa contra erro de datas)\n",
        "hoje = pd.Timestamp.now().normalize()\n",
        "antes = len(df_final)\n",
        "df_final = df_final[df_final['Date'] <= hoje]\n",
        "print(f\"üìÖ Jogos removidos (futuro/datas erradas): {antes - len(df_final)}\")\n",
        "\n",
        "# Ordenar Cronologicamente\n",
        "df = df_final.sort_values(['Date', 'HomeTeam', 'AwayTeam']).reset_index(drop=True)\n",
        "\n",
        "# Estat√≠stica\n",
        "missing_count = df['Home_xG'].isna().sum()\n",
        "print(f\"‚úÖ Merge conclu√≠do! Jogos com xG: {len(df) - missing_count} / {len(df)}\")\n",
        "print(f\"üìâ Jogos sem xG (Preenchidos com 1.0): {missing_count}\")\n",
        "\n",
        "# Preencher vazios\n",
        "df = df.fillna({'Home_xG': 1.0, 'Away_xG': 1.0})\n",
        "\n",
        "print(\"üîç A verificar duplicados no final:\")\n",
        "display(df.tail(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53a02a0a",
      "metadata": {},
      "source": [
        "## 2. Feature Engineering Completa (ELO + Stats + Odds)\n",
        "\n",
        "Aqui adicionamos as colunas B365H, B365D, B365A (Odds da Bet365)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f923a5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_features_corrected(df, window=5):\n",
        "    print(f\"üìä INICIO: {len(df)} jogos recebidos.\")\n",
        "    df = df.copy()\n",
        "    \n",
        "    # 1. LIMPEZA DE DATAS E ORDEM\n",
        "    df['Date'] = pd.to_datetime(df['Date']).dt.normalize()\n",
        "    hoje = pd.Timestamp.now().normalize()\n",
        "    \n",
        "    # Remover jogos do futuro\n",
        "    df_clean = df[df['Date'] <= hoje].copy()\n",
        "    print(f\"üìâ P√≥s-filtro de data (at√© hoje): {len(df_clean)} jogos.\")\n",
        "    \n",
        "    # Ordenar cronologicamente (CRUCIAL para o shift funcionar bem)\n",
        "    df_clean = df_clean.sort_values(['Date', 'HomeTeam', 'AwayTeam']).reset_index(drop=True)\n",
        "\n",
        "    # 2. ELO CALCULATION\n",
        "    elo_dict = {}\n",
        "    df_clean['HomeElo'] = 1500.0\n",
        "    df_clean['AwayElo'] = 1500.0\n",
        "    k_factor = 20\n",
        "    \n",
        "    for i, row in df_clean.iterrows():\n",
        "        h, a, res = row['HomeTeam'], row['AwayTeam'], row['FTR']\n",
        "        h_elo = elo_dict.get(h, 1500.0)\n",
        "        a_elo = elo_dict.get(a, 1500.0)\n",
        "        \n",
        "        df_clean.at[i, 'HomeElo'] = h_elo\n",
        "        df_clean.at[i, 'AwayElo'] = a_elo\n",
        "        \n",
        "        if res == 'H': val = 1\n",
        "        elif res == 'D': val = 0.5\n",
        "        else: val = 0\n",
        "        \n",
        "        exp_h = 1 / (1 + 10**((a_elo - h_elo)/400))\n",
        "        new_h = h_elo + k_factor * (val - exp_h)\n",
        "        new_a = a_elo + k_factor * ((1-val) - (1-exp_h))\n",
        "        \n",
        "        elo_dict[h] = new_h\n",
        "        elo_dict[a] = new_a\n",
        "        \n",
        "    df_clean['EloDiff'] = df_clean['HomeElo'] - df_clean['AwayElo']\n",
        "    \n",
        "    # 3. ROLLING STATS\n",
        "    # Preencher NaNs nas colunas originais antes de calcular m√©dias\n",
        "    cols_origin = ['FTHG', 'FTAG', 'HS', 'HST', 'HC', 'Home_xG', 'Away_xG']\n",
        "    for col in cols_origin:\n",
        "        if col in df_clean.columns:\n",
        "            df_clean[col] = df_clean[col].fillna(0)\n",
        "\n",
        "    # Criar auxiliares de pontos (ser√£o removidas no fim)\n",
        "    df_clean['Home_Points_Actual'] = df_clean['FTR'].map({'H':3, 'D':1, 'A':0})\n",
        "    df_clean['Away_Points_Actual'] = df_clean['FTR'].map({'A':3, 'D':1, 'H':0})\n",
        "    \n",
        "    # Preparar DataFrame unificado para calcular m√©dias\n",
        "    cols_home = ['Date', 'HomeTeam', 'FTHG', 'FTAG', 'HS', 'HST', 'HC', 'Home_xG', 'Home_Points_Actual']\n",
        "    cols_away = ['Date', 'AwayTeam', 'FTAG', 'FTHG', 'AS', 'AST', 'AC', 'Away_xG', 'Away_Points_Actual']\n",
        "    \n",
        "    # Filtra colunas que realmente existem\n",
        "    cols_home = [c for c in cols_home if c in df_clean.columns]\n",
        "    cols_away = [c for c in cols_away if c in df_clean.columns]\n",
        "\n",
        "    home_stats = df_clean[cols_home].rename(columns={\n",
        "        'HomeTeam':'Team', 'FTHG':'Goals', 'FTAG':'Conceded', \n",
        "        'HS':'Shots', 'HST':'SoT', 'HC':'Corners', 'Home_xG':'xG', 'Home_Points_Actual':'Points'\n",
        "    })\n",
        "    \n",
        "    away_stats = df_clean[cols_away].rename(columns={\n",
        "        'AwayTeam':'Team', 'FTAG':'Goals', 'FTHG':'Conceded', \n",
        "        'AS':'Shots', 'AST':'SoT', 'AC':'Corners', 'Away_xG':'xG', 'Away_Points_Actual':'Points'\n",
        "    })\n",
        "    \n",
        "    all_stats = pd.concat([home_stats, away_stats]).sort_values(['Team', 'Date'])\n",
        "    \n",
        "    metrics = ['Points', 'Goals', 'Conceded', 'Shots', 'SoT', 'Corners', 'xG']\n",
        "    \n",
        "    # Calcular m√©dias m√≥veis (com shift para n√£o ver o futuro)\n",
        "    for m in metrics:\n",
        "        if m in all_stats.columns:\n",
        "            all_stats[f'Avg_{m}'] = all_stats.groupby('Team')[m].transform(\n",
        "                lambda x: x.shift(1).rolling(window=window, min_periods=3).mean()\n",
        "            )\n",
        "        else:\n",
        "            all_stats[f'Avg_{m}'] = 0\n",
        "\n",
        "    # --- CORRE√á√ÉO DO ERRO ---\n",
        "    # Antes do merge, REMOVEMOS as colunas originais que v√£o colidir ou causar Leakage\n",
        "    # Isto impede o erro \"Columns must be same length\" e impede que o modelo veja o xG do jogo atual\n",
        "    cols_to_ban = ['Home_xG', 'Away_xG', 'Home_Points_Actual', 'Away_Points_Actual']\n",
        "    df_clean = df_clean.drop(columns=[c for c in cols_to_ban if c in df_clean.columns])\n",
        "\n",
        "    # Merge das m√©dias (Agora seguro, sem duplicados)\n",
        "    df_clean = df_clean.merge(all_stats[['Date', 'Team'] + [f'Avg_{m}' for m in metrics]],\n",
        "                  left_on=['Date', 'HomeTeam'], right_on=['Date', 'Team'], how='left')\n",
        "    df_clean = df_clean.drop(columns=['Team']).rename(columns={f'Avg_{m}': f'Home_{m}' for m in metrics})\n",
        "    \n",
        "    df_clean = df_clean.merge(all_stats[['Date', 'Team'] + [f'Avg_{m}' for m in metrics]],\n",
        "                  left_on=['Date', 'AwayTeam'], right_on=['Date', 'Team'], how='left')\n",
        "    df_clean = df_clean.drop(columns=['Team']).rename(columns={f'Avg_{m}': f'Away_{m}' for m in metrics})\n",
        "\n",
        "    # 4. ODDS\n",
        "    if 'B365H' in df_clean.columns:\n",
        "        df_clean['Prob_Home'] = 1 / df_clean['B365H']\n",
        "        df_clean['Prob_Draw'] = 1 / df_clean['B365D']\n",
        "        df_clean['Prob_Away'] = 1 / df_clean['B365A']\n",
        "    \n",
        "    # Preencher vazios nas features novas\n",
        "    features_cols = [c for c in df_clean.columns if 'Home_' in c or 'Away_' in c]\n",
        "    # Remover duplicados da lista se houver (seguran√ßa extra)\n",
        "    features_cols = list(set(features_cols))\n",
        "    \n",
        "    df_clean[features_cols] = df_clean[features_cols].fillna(0)\n",
        "    \n",
        "    # Limpeza final (apenas linhas sem Odd ou Resultado)\n",
        "    if 'Prob_Home' in df_clean.columns:\n",
        "        before_drop = len(df_clean)\n",
        "        df_clean = df_clean.dropna(subset=['Prob_Home', 'FTR'])\n",
        "        print(f\"üìâ Removidos {before_drop - len(df_clean)} jogos sem Odds/Resultado.\")\n",
        "\n",
        "    print(f\"‚úÖ FINAL: {len(df_clean)} jogos prontos para treino.\")\n",
        "    return df_clean, elo_dict\n",
        "\n",
        "# Executar a nova prepara√ß√£o\n",
        "df_processed, elo_tracker = prepare_features_corrected(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97f5a89c",
      "metadata": {},
      "source": [
        "## 3. Prepara√ß√£o e Treino do Modelo\n",
        "Treino Intensivo: Grid Search (Hyperparameter Tuning) Aqui √© onde \"apertamos\" o modelo. Vamos testar v√°rias combina√ß√µes. Nota: Isto pode demorar 2 ou 3 minutos a correr."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2e9837",
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√âLULA DE TREINO (CORRIGIDA)\n",
        "MODEL_FILE = 'model_xgboost_clean.pkl'\n",
        "ENCODER_FILE = 'label_encoder.pkl'\n",
        "\n",
        "# VERIFICA√á√ÉO DE SEGURAN√áA\n",
        "print(f\"üõ†Ô∏è Dados dispon√≠veis para treino: {len(df_processed)}\")\n",
        "if len(df_processed) < 100:\n",
        "    raise ValueError(\"‚ùå ERRO CR√çTICO: Tens menos de 100 jogos. O treino vai falhar. Verifica os filtros de data e o ficheiro CSV.\")\n",
        "\n",
        "\n",
        "# 1. Definir Features (REMOVER NOMES DAS EQUIPAS)\n",
        "cols_candidates = ['HomeElo', 'AwayElo', 'EloDiff', 'Prob_Home', 'Prob_Draw', 'Prob_Away'] + \\\n",
        "                  [c for c in df_processed.columns if 'Home_' in c or 'Away_' in c]\n",
        "\n",
        "# Filtro de Seguran√ßa: Remover Nomes e Datas das features\n",
        "features = [f for f in cols_candidates if f in df_processed.columns]\n",
        "features = [f for f in features if f not in ['HomeTeam', 'AwayTeam', 'Date', 'Season', 'Referee']]\n",
        "\n",
        "print(f\"üöÄ Features finais ({len(features)}): {features}\")\n",
        "\n",
        "le = LabelEncoder()\n",
        "df_processed['Target'] = le.fit_transform(df_processed['FTR'])\n",
        "\n",
        "# Split\n",
        "split = int(len(df_processed) * 0.90)\n",
        "train = df_processed.iloc[:split]\n",
        "test = df_processed.iloc[split:]\n",
        "X_train, y_train = train[features], train['Target']\n",
        "X_test, y_test = test[features], test['Target']\n",
        "\n",
        "# Grid Search (Treino)\n",
        "print(\"‚ö†Ô∏è A iniciar treino sem 'nomes' (evita overfitting)...\")\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(random_state=42, objective='multi:softprob', eval_metric='mlogloss')\n",
        "param_grid = {\n",
        "    'n_estimators': [150, 200],\n",
        "    'max_depth': [3, 4],\n",
        "    'learning_rate': [0.03, 0.05],\n",
        "    'gamma': [0, 0.1],\n",
        "    'min_child_weight': [1, 3]\n",
        "}\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "grid = GridSearchCV(xgb_model, param_grid, cv=tscv, scoring='accuracy', verbose=1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid.best_params_\n",
        "print(f\"‚úÖ Melhores par√¢metros: {best_params}\")\n",
        "\n",
        "# Treino Final com Pesos\n",
        "weights = np.ones(len(y_train))\n",
        "draw_idx = le.transform(['D'])[0]\n",
        "weights[y_train == draw_idx] = 1.3\n",
        "\n",
        "print(\"‚öñÔ∏è A treinar modelo final...\")\n",
        "model_final = xgb.XGBClassifier(**best_params, random_state=42, objective='multi:softprob')\n",
        "model_final.fit(X_train, y_train, sample_weight=weights)\n",
        "\n",
        "joblib.dump(model_final, MODEL_FILE)\n",
        "joblib.dump(le, ENCODER_FILE)\n",
        "print(\"üíæ Modelo salvo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da910c6c",
      "metadata": {},
      "source": [
        "### Matriz de Confus√£o e accuracy\n",
        "Vamos ver visualmente onde o modelo erra.\n",
        "* Eixo Y: O que realmente aconteceu.\n",
        "* Eixo X: O que o modelo previu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2715419f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Avalia√ß√£o\n",
        "preds = model_final.predict(X_test)\n",
        "acc = accuracy_score(y_test, preds)\n",
        "print(f\"üéØ Accuracy Final (com xG): {acc:.2%}\")\n",
        "\n",
        "# Ver import√¢ncia das features\n",
        "importances = pd.Series(model_final.feature_importances_, index=features).sort_values(ascending=False)\n",
        "print(\"\\nTop 5 Fatores mais importantes:\")\n",
        "print(importances.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd60024b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def diagnose_model(df, model, features, target_col='Target'):\n",
        "    print(\"--- üïµÔ∏è‚Äç‚ôÇÔ∏è RELAT√ìRIO DE DIAGN√ìSTICO ---\")\n",
        "    \n",
        "    # 1. VERIFICAR DATA LEAKAGE (A mais importante!)\n",
        "    # Vamos ver a correla√ß√£o de TODAS as features com o Target (Resultado)\n",
        "    # Se alguma feature tiver correla√ß√£o > 0.8, √© prov√°vel que seja leakage (spoiler do resultado)\n",
        "    print(\"\\nüîç 1. An√°lise de Data Leakage (Correla√ß√µes Suspeitas)\")\n",
        "    df_corr = df[features + [target_col]].copy()\n",
        "    corr_matrix = df_corr.corr()\n",
        "    target_corr = corr_matrix[target_col].abs().sort_values(ascending=False).drop(target_col)\n",
        "    \n",
        "    top_suspects = target_corr.head(10)\n",
        "    print(top_suspects)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=top_suspects.values, y=top_suspects.index, palette='Reds_r')\n",
        "    plt.title(\"Top Correla√ß√µes com o Resultado (Aten√ß√£o se > 0.6)\")\n",
        "    plt.axvline(x=0.6, color='black', linestyle='--', label='Zona de Perigo')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. MATRIZ DE CONFUS√ÉO VISUAL\n",
        "    print(\"\\nüé® 2. Performance Real (Matriz de Confus√£o)\")\n",
        "    # Split manual r√°pido para teste (usando o mesmo split do teu c√≥digo)\n",
        "    split = int(len(df) * 0.90)\n",
        "    X_test = df.iloc[split:][features]\n",
        "    y_test = df.iloc[split:][target_col]\n",
        "    \n",
        "    preds = model.predict(X_test)\n",
        "    \n",
        "    # Mapear de volta para nomes (0,1,2 -> A, D, H) se usaste LabelEncoder\n",
        "    # Assumindo ordem alfab√©tica t√≠pica do LabelEncoder: 0=Away, 1=Draw, 2=Home\n",
        "    labels = ['Away', 'Draw', 'Home']\n",
        "    \n",
        "    conf_mat = confusion_matrix(y_test, preds)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Previs√£o do Modelo')\n",
        "    plt.ylabel('O que Realmente Aconteceu')\n",
        "    plt.title('Onde √© que o modelo est√° a errar?')\n",
        "    plt.show()\n",
        "\n",
        "    # 3. CHECAR DADOS HIST√ìRICOS (ELO e xG)\n",
        "    print(\"\\nüìà 3. Verifica√ß√£o Visual de Dados (Elo e xG)\")\n",
        "    teams_check = ['Aston Villa', 'Arsenal']  # coloca aqui as equipas\n",
        "    colors = ['blue', 'red']  # podes mudar as cores se quiseres\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    for team_check, color in zip(teams_check, colors):\n",
        "        # Filtrar jogos da equipa\n",
        "        df_team = df[(df['HomeTeam'] == team_check) | (df['AwayTeam'] == team_check)].sort_values('Date')\n",
        "\n",
        "        # Escolher o Elo conforme casa/fora\n",
        "        df_team['MyElo'] = np.where(\n",
        "            df_team['HomeTeam'] == team_check,\n",
        "            df_team['HomeElo'],\n",
        "            df_team['AwayElo']\n",
        "        )\n",
        "\n",
        "        # Plot\n",
        "        plt.plot(df_team['Date'], df_team['MyElo'], label=f'Elo {team_check}', color=color)\n",
        "\n",
        "    plt.title(\"Evolu√ß√£o do Elo das Equipas (deve variar suavemente)\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Corre o diagn√≥stico\n",
        "diagnose_model(df_processed, model_final, features)\n",
        "\n",
        "# --- DEBUG ESPEC√çFICO DO JOGO VILLA VS ARSENAL ---\n",
        "def inspect_prediction_input(home, away):\n",
        "    print(f\"\\nüî¨ RAIO-X: {home} vs {away}\")\n",
        "    \n",
        "    # Tenta encontrar a √∫ltima linha de dados usada\n",
        "    try:\n",
        "        h_row = df_processed[df_processed['HomeTeam'] == home].iloc[-1]\n",
        "        a_row = df_processed[df_processed['AwayTeam'] == away].iloc[-1]\n",
        "        \n",
        "        print(f\"\\nDados crus extra√≠dos do hist√≥rico para {home}:\")\n",
        "        cols_to_show = ['Date', 'HomeElo', 'Home_xG', 'Home_Goals', 'Home_Shots']\n",
        "        # Filtra colunas que existem\n",
        "        cols_exist = [c for c in cols_to_show if c in df_processed.columns]\n",
        "        print(h_row[cols_exist])\n",
        "        \n",
        "        print(f\"\\nDados crus extra√≠dos do hist√≥rico para {away}:\")\n",
        "        print(a_row[cols_exist])\n",
        "        \n",
        "        # Verificar se h√° zeros suspeitos (ex: xG = 0.0 √© estranho numa m√©dia)\n",
        "        if h_row.get('Home_xG', 0) == 0 or a_row.get('Away_xG', 0) == 0:\n",
        "            print(\"\\n‚ö†Ô∏è ALERTA: Foi detetado xG = 0. O Scraper pode ter falhado para esta equipa!\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao inspecionar: {e}\")\n",
        "\n",
        "inspect_prediction_input('Aston Villa', 'Arsenal')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7176fed1",
      "metadata": {},
      "source": [
        "## 4. Aplica√ß√£o na \"Vida Real\"\n",
        "Aqui est√° a fun√ß√£o final. Ela usa o dicion√°rio `current_elo` (que cont√©m os valores mais recentes ap√≥s o √∫ltimo jogo do dataset) para fazer previs√µes sobre jogos futuros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae7c1481",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_smart(home, away, odd_h, odd_d, odd_a):\n",
        "    # --- 1. CARREGAMENTO DO MODELO ---\n",
        "    # Tenta usar o modelo que est√° na mem√≥ria (model_final).\n",
        "    global model_final, le # Garante que acedemos √†s vari√°veis globais se existirem\n",
        "    \n",
        "    try:\n",
        "        model = model_final\n",
        "    except NameError:\n",
        "        print(\"‚ö†Ô∏è Modelo n√£o est√° na mem√≥ria. A carregar do disco...\")\n",
        "        model = joblib.load('model_xgboost_xg.pkl') # <--- O NOME CORRETO √â ESTE\n",
        "        \n",
        "    # Verificar se o LabelEncoder (le) existe, sen√£o carregar\n",
        "    try:\n",
        "        encoder = le\n",
        "    except NameError:\n",
        "        encoder = joblib.load('label_encoder.pkl')\n",
        "\n",
        "    # --- 2. PREPARAR DADOS ---\n",
        "    # (Assume que elo_tracker e df_processed est√£o em mem√≥ria)\n",
        "    h_elo = elo_tracker.get(home, 1500)\n",
        "    a_elo = elo_tracker.get(away, 1500)\n",
        "    \n",
        "    input_data = {\n",
        "        'HomeElo': h_elo, 'AwayElo': a_elo, 'EloDiff': h_elo - a_elo,\n",
        "        'Prob_Home': 1/odd_h, 'Prob_Draw': 1/odd_d, 'Prob_Away': 1/odd_a\n",
        "    }\n",
        "    \n",
        "    # Preencher stats hist√≥ricas (xG, Remates, etc.)\n",
        "    # Se a equipa subiu de divis√£o agora e n√£o tem hist√≥rico, usamos zeros (seguran√ßa)\n",
        "    try:\n",
        "        h_row = df_processed[df_processed['HomeTeam'] == home].iloc[-1]\n",
        "    except IndexError:\n",
        "        h_row = pd.Series(0, index=df_processed.columns)\n",
        "        \n",
        "    try:\n",
        "        a_row = df_processed[df_processed['AwayTeam'] == away].iloc[-1]\n",
        "    except IndexError:\n",
        "        a_row = pd.Series(0, index=df_processed.columns)\n",
        "    \n",
        "    # Preencher as features necess√°rias\n",
        "    for feat in features:\n",
        "        if feat not in input_data:\n",
        "            if 'Home_' in feat: input_data[feat] = h_row.get(feat, 0)\n",
        "            elif 'Away_' in feat: input_data[feat] = a_row.get(feat, 0)\n",
        "\n",
        "    X_input = pd.DataFrame([input_data])[features]\n",
        "    \n",
        "    # --- 3. PREVIS√ÉO ---\n",
        "    probs = model.predict_proba(X_input)[0]\n",
        "    \n",
        "    # Mapear probabilidades corretamente usando o encoder\n",
        "    # O encoder sabe que 0=Away, 1=Draw, 2=Home (ou a ordem alfab√©tica 'A', 'D', 'H')\n",
        "    class_order = encoder.classes_\n",
        "    prob_dict = {class_label: prob for class_label, prob in zip(class_order, probs)}\n",
        "    \n",
        "    p_home = prob_dict.get('H', 0)\n",
        "    p_draw = prob_dict.get('D', 0)\n",
        "    p_away = prob_dict.get('A', 0)\n",
        "    \n",
        "    print(f\"\\nüß† An√°lise: {home} vs {away}\")\n",
        "    print(f\"   Probabilidades: Casa {p_home:.1%} | Empate {p_draw:.1%} | Fora {p_away:.1%}\")\n",
        "    print(f\"   (xG M√©dio Recente: {home} {input_data.get('Home_xG',0):.2f} vs {away} {input_data.get('Away_xG',0):.2f})\")\n",
        "    \n",
        "    # --- 4. VEREDICTO ---\n",
        "    prediction = \"Inconclusivo\"\n",
        "    \n",
        "    if p_home > 0.45:\n",
        "        prediction = f\"Vit√≥ria {home}\"\n",
        "    elif p_away > 0.45:\n",
        "        prediction = f\"Vit√≥ria {away}\"\n",
        "    elif p_draw > 0.28: \n",
        "        prediction = \"EMPATE (Risco calculado)\"\n",
        "    else:\n",
        "        max_prob = max(p_home, p_draw, p_away)\n",
        "        if max_prob == p_home: prediction = f\"Tend√™ncia {home}\"\n",
        "        elif max_prob == p_away: prediction = f\"Tend√™ncia {away}\"\n",
        "        else: prediction = \"Tend√™ncia Empate\"\n",
        "\n",
        "    print(f\"   >> Veredicto IA: {prediction}\")\n",
        "\n",
        "# Testa com o jogo que querias\n",
        "predict_smart('Aston Villa', 'Arsenal', 4.05, 3.45, 1.84)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
